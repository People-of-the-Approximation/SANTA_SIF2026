# ğŸ… SANTA: Scalable Accelerator for Nonlinear function with Tree-based Architecture

> **[2026 POLARIS Semiconductor Innovation Festival (SIF)](https://polargate.disu.ac.kr/contest/SIF2026/winner?sc=y)**
> Team-People of the Approximation (ê·¼ì‚¬í•œì‚¬ëŒë“¤)

## Introduction
SANTA is an FPGA-based hardware accelerator designed to speed up the Softmax nonlinear function, a key bottleneck in Transformer-based language models (BERT, GPT, etc.).

Modern AI services must handle a wide range of input lengths, from short queries (e.g., voice assistants) to long contexts (e.g., LLMs).
Conventional fixed-size accelerators either waste computation on short inputs due to unnecessary padding, or fail to support long inputs beyond their fixed capacity.

**SANTA** addresses these limitations using a **Tree-based Architecture** and **Forwarding Logic**.
- Speed: Maximizes processing speed for short queries by parallelizing the computation.
- Scalability: Flexibly supports long inputs up to 768 tokens beyond the physical hardware size constraint.

## Key Features

### 1. Tree-Bypass Logic (Parallel Acceleration)
Dynamically reconfigures hardware resources according to input length.
- Variable-length modes: Operates in `16x4`, `32x2`, and `64x1` modes depending on the input length.
- Parallel processing: For short sentences with 16 tokens or fewer, it runs
  four Softmax computations in parallel, improving throughput by **2.2Ã—** (SST-2 Validation Set).

### 2. Forwarding Logic (Infinite Scalability)
A structure that can process long sequences beyond the physical buffer size (64-input).
- Local-to-Global Update: For each data batch, the locally computed Max/Sum values are forwarded to the next batch to derive the global Softmax result.
- Flexibility: This enables processing up to the maximum input length of GPT/BERT (768 tokens), and in theory, unlimited sequence lengths.

### 3. Hardware-Efficient Approximation (RU)
Optimizes expensive exponential and division operations into hardware-friendly forms.
- Q6.10 Fixed-Point: Uses fixed-point instead of floating-point to reduce hardware cost.
- Base-2 Transformation: Uses base-2 logarithm and exponential instead of natural log/exp, replacing complex operations with simple shift-and-add.
- Resource Efficiency: Achieves **50%** LUT/FF reduction and **27%** power reduction compared to an FP16 design, and uses **zero DSP slices**.

## System Architecture

The overall system operates via a Host-Accelerator hybrid flow:
1.  Host PC (Web UI): Receives user text input (GPT-2/BERT).
2.  Softmax HW API: Extracts only the Softmax operation from the PyTorch attention layer and sends it over UART.
3.  SANTA Chip (FPGA):
    - UART Module: Receives and buffers data.
    - Core: Max Tree -> Forwarding -> RU (Exp) -> Adder Tree -> Forwarding -> RU (Div).
4.  Output: Returns the results to the Host, which then produces the final text generation or sentiment classification output.

| Component | Specification |
|:---:|:---|
| Board | Nexys A7-100T (Xilinx Artix-7) |
| Interface | UART (Universal Asynchronous Receiver-Transmitter) |
| Frequency | 100 MHz |
| Precision | Fixed-Point Q6.10 |

## Performance Evaluation

Verification was conducted using a BERT-Base model and the SST-2 (Stanford Sentiment Treebank) dataset.

| Metric | SW (PyTorch FP32) | HW (SANTA Q6.10) | Note |
|:---|:---:|:---:|:---|
| Accuracy | 92.4% | 92.2% | Maintains FP32-level accuracy |
| Agreement | - | 99.8% | 99.8% match with float SW outputs |
| Throughput | 1.0x (Baseline) | 2.2x | Acceleration for short sentences (Avg 25 tokens) |

## Demo
*In the real demo environment, a Python FastAPI-based web interface was used to visualize the hardware operation.*

| Model | Input Example | Result |
|:---|:---|:---|
| Text Generation (GPT-2) | `Hi nice to` | Generates `meet you.` (Hardware-accelerated) |
| Sentiment Analysis (BERT) | `I love you` | Classified as POSITIVE |

## Team-People of the Approximation (ê·¼ì‚¬í•œì‚¬ëŒë“¤)
- Sanghyeok Park (ë°•ìƒí˜)
  - **Leader**
  - Idea Conception
  - System Architecture
  - RTL Design
  - Approximation Algorithm
  - FPGA Implementation
  - Verification
  - Software Integration
  - Host API Development
  - Figure Illustrations
- Sang-yoon Kim (ê¹€ìƒìœ¤)
  - RTL Design
  - UI/UX Design
  - Web API Development
  - Software Integration
  - Figure Illustrations
- Seo-yoon Jang (ì¥ì„œìœ¤)
  - RTL Design
  - UI/UX Design
  - Figure Illustrations
  - Documentation

---
*This project was submitted to POLARIS SIF 2026.*

---

# Korean Version: ğŸ… SANTA: íŠ¸ë¦¬ ê¸°ë°˜ ê³ í™•ì¥ì„± ë¹„ì„ í˜• ì—°ì‚° ê°€ì†ê¸° ì„¤ê³„ ë° ê²€ì¦

> **[2026 POLARIS Semiconductor Innovation Festival (SIF)](https://polargate.disu.ac.kr/contest/SIF2026/winner?sc=y)**
> Team-ê·¼ì‚¬í•œì‚¬ëŒë“¤ (People of the Approximation)

## Introduction
SANTAëŠ” Transformer ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸(BERT, GPT ë“±)ì˜ í•µì‹¬ ë³‘ëª© êµ¬ê°„ì¸ 
Softmax Nonlinear functionì„ ê°€ì†í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ FPGA ê¸°ë°˜ì˜ Hardware Acceleratorì…ë‹ˆë‹¤.

í˜„ëŒ€ì˜ AI ì„œë¹„ìŠ¤ëŠ” ìŒì„± ë¹„ì„œì™€ ê°™ì€ ì§§ì€ ì¿¼ë¦¬(Short Query)ë¶€í„° 
LLMê³¼ ê°™ì€ ê¸´ ë¬¸ë§¥(Long Context)ê¹Œì§€ ë‹¤ì–‘í•œ ì…ë ¥ ê¸¸ì´ë¥¼ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. 
ê¸°ì¡´ì˜ ê³ ì •ëœ ì…ë ¥ í¬ê¸°(Fixed-size)ë¥¼ ê°€ì§„ Acceleratorë“¤ì€ ì§§ì€ ì…ë ¥ì— ëŒ€í•´ ë¶ˆí•„ìš”í•œ íŒ¨ë”© ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê±°ë‚˜, 
ê¸´ ì…ë ¥ì— ëŒ€ì‘í•˜ì§€ ëª»í•˜ëŠ” í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

**SANTA**ëŠ” **Tree-based Architecture**ì™€ **Forwarding Logic**ì„ í†µí•´ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
- Speed: ì§§ì€ ì¿¼ë¦¬ì— ëŒ€í•´ ì—°ì‚°ì„ ë³‘ë ¬í™”í•˜ì—¬ ì²˜ë¦¬ ì†ë„ë¥¼ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
- Scalability: Hardwareì˜ ë¬¼ë¦¬ì  í¬ê¸° ì œì•½ì„ ë„˜ì–´, ìµœëŒ€ 768 í† í°ì˜ ê¸´ ì…ë ¥ê¹Œì§€ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.

## Key Features

### 1. Tree-Bypass Logic (Parallel Acceleration)
ì…ë ¥ ê¸¸ì´ì— ë”°ë¼ Hardware ë¦¬ì†ŒìŠ¤ë¥¼ ë™ì ìœ¼ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.
- ê°€ë³€ ëª¨ë“œ ì§€ì›: ì…ë ¥ ê¸¸ì´ì— ë”°ë¼ `16x4`, `32x2`, `64x1` ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤.
- ë³‘ë ¬ ì²˜ë¦¬: 16 í† í° ì´í•˜ì˜ ì§§ì€ ë¬¸ì¥ì´ ë“¤ì–´ì˜¬ ê²½ìš°, 
4ê°œì˜ Softmax ì—°ì‚°ì„ ë™ì‹œì— ë³‘ë ¬ ìˆ˜í–‰í•˜ì—¬ ì²˜ë¦¬ëŸ‰(Throughput)ì„ 2.2ë°° í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤ (SST-2 Validation Set).

### 2. Forwarding Logic (Infinite Scalability)
ë¬¼ë¦¬ì ì¸ ë²„í¼ í¬ê¸°(64-input)ë¥¼ ì´ˆê³¼í•˜ëŠ” ê¸´ ë¬¸ì¥ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.
- Local-to-Global Update: ë°ì´í„° ë¬¶ìŒ(Batch)ë§ˆë‹¤ ê³„ì‚°ëœ Local Max/Sum ê°’ì„ ë‹¤ìŒ ì—°ì‚°ìœ¼ë¡œ ì „ë‹¬(Forwarding)í•˜ì—¬ Global Softmax ê°’ì„ ë„ì¶œí•©ë‹ˆë‹¤.
- ìœ ì—°ì„±: ì´ë¥¼ í†µí•´ GPT/BERTì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ì¸ 768 í† í°ì„ í¬í•¨, ì´ë¡ ìƒ ë¬´ì œí•œì˜ ê¸¸ì´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 3. Hardware-Efficient Approximation (RU)
ë³µì¡í•œ exponential í•¨ìˆ˜ì™€ ë‚˜ëˆ—ì…ˆ ì—°ì‚°ì„ Hardware ì¹œí™”ì ìœ¼ë¡œ ìµœì í™”í–ˆìŠµë‹ˆë‹¤.
- Q6.10 Fixed-Point: ë¶€ë™ì†Œìˆ˜ì  ëŒ€ì‹  ê³ ì •ì†Œìˆ˜ì ì„ ì‚¬ìš©í•˜ì—¬ ì—°ì‚° ë¹„ìš©ì„ ì ˆê°í–ˆìŠµë‹ˆë‹¤.
- Base-2 Transformation: ìì—°ë¡œê·¸ ëŒ€ì‹  base-2 logì™€ exponentialë¥¼ ì‚¬ìš©í•˜ì—¬, ë³µì¡í•œ ì—°ì‚°ì„ ë‹¨ìˆœ Shiftì™€ Addë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.
- Resource Efficiency: FP16 ëª¨ë¸ ëŒ€ë¹„ LUT/FF ì‚¬ìš©ëŸ‰ 50% ì ˆê°, ì „ë ¥ ì†Œëª¨ 27% ê°ì†Œë¥¼ ë‹¬ì„±í–ˆìœ¼ë©° DSP ìŠ¬ë¼ì´ìŠ¤ë¥¼ ì „í˜€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

## System Architecture

The overall system operates via a Host-Accelerator hybrid flow:
1.  Host PC (Web UI): ì‚¬ìš©ìë¡œë¶€í„° í…ìŠ¤íŠ¸(GPT-2/BERT) ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤.
2.  Softmax HW API: PyTorch ëª¨ë¸ì˜ Attention ë ˆì´ì–´ì—ì„œ Softmax ì—°ì‚°ë§Œ ì¶”ì¶œí•˜ì—¬ UARTë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
3.  SANTA Chip (FPGA):
    - UART Module: ë°ì´í„° ìˆ˜ì‹  ë° ë²„í¼ë§.
    - Core: Max Tree -> Forwarding -> RU (Exp) -> Adder Tree -> Forwarding -> RU (Div).
4.  Output: ì—°ì‚° ê²°ê³¼ë¥¼ Hostë¡œ ë°˜í™˜í•˜ì—¬ ìµœì¢… í…ìŠ¤íŠ¸ ìƒì„± ë˜ëŠ” ê°ì • ë¶„ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

| Component | Specification |
|:---:|:---|
| Board | Nexys A7-100T (Xilinx Artix-7) |
| Interface | UART (Universal Asynchronous Receiver-Transmitter) |
| Frequency | 100 MHz |
| Precision | Fixed-Point Q6.10 |

## Performance Evaluation

ê²€ì¦ì€ BERT-Base ëª¨ë¸ê³¼ SST-2 (Stanford Sentiment Treebank) ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.

| Metric | SW (PyTorch FP32) | HW (SANTA Q6.10) | Note |
|:---|:---:|:---:|:---|
| Accuracy | 92.4% | 92.2% | FP32 ëª¨ë¸ê³¼ ë™ë“± ìˆ˜ì¤€ì˜ ì •í™•ë„ ìœ ì§€ |
| Agreement | - | 99.8% | Float SW ëª¨ë¸ê³¼ ê²°ê³¼ê°’ 99.8% ì¼ì¹˜ |
| Throughput | 1.0x (Baseline) | 2.2x | ì§§ì€ ë¬¸ì¥(Avg 25 tokens) ì²˜ë¦¬ ì‹œ ê°€ì† íš¨ê³¼ |

## Demo
*ì‹¤ì œ ì‹œì—° í™˜ê²½ì—ì„œëŠ” Python FastAPI ê¸°ë°˜ì˜ Web ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ Hardware ë™ì‘ì„ ì‹œê°í™”í•˜ì˜€ìŠµë‹ˆë‹¤.*

| Model | Input Example | Result |
|:---|:---|:---|
| Text Generation (GPT-2) | `Hi nice to` ì…ë ¥ | `meet you.` ìƒì„± (Hardware ê°€ì†) |
| Sentiment Analysis (BERT) | `I love you` ì…ë ¥ | POSITIVE íŒë³„ |

## Team-ê·¼ì‚¬í•œì‚¬ëŒë“¤ (People of the Approximation)
- ë°•ìƒí˜ (Sanghyeok Park)
  - **Leader**
  - Idea Conception
  - System Architecture
  - RTL Design
  - Approximation Algorithm
  - FPGA Implementation
  - Verification
  - Software Integration
  - Host API Development
  - Figure Illustrations
- ê¹€ìƒìœ¤ (Sang-yoon Kim)
  - RTL Design
  - UI/UX Design
  - Web API Development
  - Software Integration
  - Figure Illustrations
- ì¥ì„œìœ¤ (Seo-yoon Jang)
  - RTL Design
  - UI/UX Design
  - Figure Illustrations
  - Documentation

---
*This project was submitted to POLARIS SIF 2026.*